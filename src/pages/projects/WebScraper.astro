---
import Layout from '../../layout/Layout.astro';

const project = {
  title: "Web Scraper",
  tagline: "Fanfiction discovery tool with keyword filtering and local LLM classification.",
  date: "2024",
  role: "Developer",
  repoUrl: "https://github.com/crowmoed/Webscraper---Python",
  stack: ["Python", "SeleniumBase", "Ollama", "Google Sheets API", "gspread"]
};

const githubIcon = `
<svg viewBox="0 0 24 24" width="20" height="20" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
  <path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.05-.015-2.07-3.345.72-4.05-1.605-4.05-1.605-.54-1.38-1.335-1.755-1.335-1.755-1.095-.75.09-.735.09-.735 1.215.09 1.845 1.245 1.845 1.245 1.08 1.845 2.835 1.305 3.525.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.225 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405 1.02 0 2.04.135 3 .405 2.295-1.545 3.3-1.23 3.3-1.23.66 1.695.24 2.925.12 3.225.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.285 0 .315.225.69.825.57A12.02 12.02 0 0 0 24 12c0-6.63-5.37-12-12-12z"/>
</svg>`;
---

<Layout>
  <div class="projects-wrapper back">
    
    <div class="detail-container">
      
      <header class="detail-header fade-in">
        <a href="/projects" class="back-link">
          <span class="arrow">←</span> All Projects
        </a>
        <h1>{project.title}</h1>
        <p class="tagline">{project.tagline}</p>
      </header>

      <div class="content-grid fade-in-delay">
        
        <main class="project-body">
          <div class="project-hero-image">
             <div class="placeholder-overlay">Content Filtering Pipeline</div>
          </div>

          <div class="text-content">
            <h3>What Is This?</h3>
            <p>
              A scraper that trawls AO3 and FanFiction.net looking for stories that match specific criteria. Keyword filtering handles the obvious rejects, then a local LLM decides if the rest are actually worth reading.
            </p>
            <p>
              Matches get logged to a Google Sheet for later. The whole thing runs locally—no API costs, no rate limits.
            </p>

            <h3>The Two-Stage Filter</h3>
            <p>
              LLM inference is slow. Running every story through one would take forever. So the pipeline has two stages:
            </p>
            <ol>
              <li><strong>Keyword filter</strong> — Fast substring matching against summaries. No keywords? Skip immediately.</li>
              <li><strong>LLM classification</strong> — Stories that pass stage one get sent to Ollama for a real judgment call. The model returns True or False.</li>
            </ol>
            <p>
              This cuts LLM calls by 50-90% depending on how selective your keywords are. Most stories don't make it past stage one.
            </p>

            <h3>Browser Automation</h3>
            <p>
              Standard Selenium gets blocked instantly on these sites. SeleniumBase in undetected-chromedriver mode gets around that—it masks the automation fingerprints that bot detection looks for.
            </p>
            <p>
              The scraper handles CAPTCHAs automatically via <code>uc_gui_click_captcha()</code>, and <code>uc_open_with_reconnect()</code> retries on flaky connections. DOM extraction uses CSS selectors to pull titles and summaries from each page.
            </p>

            <h3>Site-Specific Scrapers</h3>
            <p>
              Each site needs its own module because the DOM structure differs:
            </p>
            <ul>
              <li><strong>AO3</strong> — Filters by fandom tag, 80k+ word count, sorted by update date. Pulls from <code>div.header.module</code> and <code>.userstuff.summary</code>.</li>
              <li><strong>FanFiction.net</strong> — Filters by rating, length, and language. Uses <code>.stitle</code> and <code>.z-indent.z-padtop</code>.</li>
            </ul>
            <p>
              If either site changes their markup, the selectors break. That's the fragile part.
            </p>

            <h3>Local LLM</h3>
            <p>
              Classification runs through Ollama—phi4, phi4-mini, or deepseek-r1 depending on what you've got pulled. Temperature is set to 0 for deterministic outputs. The prompt tells the model to respond with a single word: True or False.
            </p>
            <p>
              Parsing is just string matching. If the model gets chatty, it breaks. But with the right prompt, it doesn't.
            </p>

            <h3>Output</h3>
            <p>
              Matches append to a Google Sheet via gspread with OAuth2 service account auth. Each row gets title, summary, page number, and URL. Easy to scan through later and pick what to actually read.
            </p>

            <h3>Resumable Runs</h3>
            <p>
              Start and end pages are configurable constants. If a scrape gets interrupted, bump the start page and run again. Not sophisticated, but it works.
            </p>
          </div>
        </main>

        <aside class="project-sidebar">
          
          <div class="meta-block">
            <span class="meta-label">Role</span>
            <span class="meta-value">{project.role}</span>
          </div>

          <div class="meta-block">
            <span class="meta-label">Timeline</span>
            <span class="meta-value">{project.date}</span>
          </div>

          <div class="meta-block">
            <span class="meta-label">Stack</span>
            <div class="tech-stack-wrap">
              {project.stack.map(tech => (
                <span class="tech-tag">{tech}</span>
              ))}
            </div>
          </div>

          <div class="meta-block">
            <span class="meta-label">Features</span>
            <div class="tech-stack-wrap">
              <span class="tech-tag">Two-Stage Filter</span>
              <span class="tech-tag">Local LLM</span>
              <span class="tech-tag">CAPTCHA Handling</span>
              <span class="tech-tag">Sheets Export</span>
            </div>
          </div>

          <div class="meta-block">
            <span class="meta-label">Sites</span>
            <span class="meta-value">AO3, FanFiction.net</span>
          </div>

          <div class="meta-block">
            <span class="meta-label">LLM</span>
            <span class="meta-value">Ollama (phi4, deepseek-r1)</span>
          </div>

          <div class="action-buttons">
            <a href={project.repoUrl} target="_blank" class="action-btn outline">
              <span class="icon" set:html={githubIcon} />
              View Code
            </a>
          </div>

        </aside>

      </div>

    </div>
  </div>
</Layout>